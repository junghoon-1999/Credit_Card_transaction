{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Team: SacSlicerII\n",
    "\n",
    "## Members:\n",
    "<b>1. 180030801</b> <br>\n",
    "<b>2. 200006496</b> <br>\n",
    "<b>3. 180016244</b> <br>\n",
    "<b>4. 180014835</b> <br>\n",
    "<b>5. 190004840</b> <br>\n",
    "<b>6. 180003171</b> <br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1.0 Data Exploration\n",
    "\n",
    "We begin by examining the structure of the datasets present. Notably, we have two distinct datasets (further separated into testing and training sets). A summary of this information can be found in the following links:\n",
    "\n",
    "1. https://www.kaggle.com/c/ieee-fraud-detection/discussion/101203\n",
    "2. https://www.kaggle.com/c/ieee-fraud-detection/data\n",
    "\n",
    "## 1.1 Transaction Dataset\n",
    "The following is a summary of the 393 features found in this dataset\n",
    "\n",
    "| Feature | Type (overview) | Type (fine) | Description |\n",
    "| --- | --- | --- | --- |\n",
    "| TransactionID | Primary Key | int | 7 Digit Primary Key |\n",
    "| TransactionDT | Quantitative | int | Timedelta Duration in Seconds |\n",
    "| TransactionAMT | Quantitative | int | USD ($) Transaction Amount (to 3 decimals) |\n",
    "| ProductCD | Categorical | str | Product Code of Transaction (single letter) |\n",
    "| card{1,2,3,5} | Categorical | int? | Payment card information (numerical) |\n",
    "| card{4,6} | Categorical | str | Payment card information (categorical) |\n",
    "| addr{1,2} | Categorical | int | Purchaser address information |\n",
    "| dist{1,2} | Quantitative | int? | Distance between addresses/IP's etc. |\n",
    "| {P,R}_emaildomain | Categorical | str | Purchaser/recipient email domain |\n",
    "| C{1-14} | Quantitative | int | Counting (context masked) |\n",
    "| D{1-15} | Quantitative | int | Timedelta such as days between transactions |\n",
    "| M{1-9} | Categorical | bool | Match such as names on cards, addresses, etc. |\n",
    "| V{1-339} | Quantitative | int/float | Vesta engineered features (all numerical) |\n",
    "\n",
    "## 1.2 Identity Dataset\n",
    "The following is a summary of the 41 features found in this dataset\n",
    "\n",
    "| Feature | Type (overview) | Type (fine) | Description |\n",
    "| --- | --- | --- | --- |\n",
    "| TransactionID | Primary Key | int | 7 Digit Primary Key |\n",
    "| id-{01-11} | Quantitative | ? | Various identity features (numerical) |\n",
    "| id-{12-38} | Categorical | ? | Various identity features (categorical) |\n",
    "| DeviceType | Categorical | str | Mobile or desktop device type |\n",
    "| DeviceInfo | Categorical | str | Device information (e.g. OS, model, etc.) |\n",
    "\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.0 Data initialization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, most of our data wrangling was inspired (although significantly different) from that found in the following link:\n",
    "1. https://www.kaggle.com/c/ieee-fraud-detection/discussion/108575\n",
    "\n",
    "We performed a left join on the transaction and identity information since not all transactions have ID information, but we still had to predict for them anyway, and since some transactions in the identity dataset did not have corresponding `isFraud` entries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Import the datsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We imported the datasets and merged `transaction` and `identity` datasets for both `test` and `train`. We then removed any columns that had more than 90% NaN entries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the necessary datasets \n",
    "data_train_t = pd.read_csv('ieee-fraud-detection/train_transaction.csv')\n",
    "data_train_i = pd.read_csv('ieee-fraud-detection/train_identity.csv')\n",
    "data_test_t = pd.read_csv('ieee-fraud-detection/test_transaction.csv')\n",
    "data_test_i = pd.read_csv('ieee-fraud-detection/test_identity.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the transaction and identity datasets together and drop columns with more than 90% NAs\n",
    "train = pd.merge(data_train_t, data_train_i, how = 'left',on = 'TransactionID')\n",
    "train.dropna(thresh = train.shape[0]*0.1, how = 'all', axis = 1, inplace = True)\n",
    "\n",
    "# Remove transactionID from the training dataset\n",
    "del train['TransactionID']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge the transaction and identity datasets together and drop columns with more than 90% NAs\n",
    "test = pd.merge(data_test_t, data_test_i, how = 'outer', on = 'TransactionID')\n",
    "\n",
    "# Fix the name of the test columns by replacing '-' with '_'\n",
    "test= test.rename(columns=lambda x:\"_\".join(x.split(\"-\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.0 Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Type Correction\n",
    "\n",
    "A key issue with unique counts is that integer values are being stored as floats, making it appear as if there are thousands of categories when in fact there are under 100. The most obvious example of this is observed in `addr2` which corresponds to countries. After converting this variable to a set, it claimed there were 65,780 unique values when in reality there are not nearly as many countries as this. Converting this variable to integers yielded 75 unique values instead.\n",
    "\n",
    "\n",
    "Categorical variable number unique values excluding NaN (remove 1 from above results)\n",
    "\n",
    "| Attribute | Unique Values | Numerical | Notes |\n",
    "| --- | --- | --- | --- |\n",
    "| ProductCD | 5 |  |\n",
    "| card1 | 13552 | Yes | Likely card ID (ALREADY INT) |\n",
    "| card2 | 500 | Yes | |\n",
    "| card3 | 114 | Yes |  |\n",
    "| card4 | 5 |  | Card provider |\n",
    "| card5 | 119 | Yes |  |\n",
    "| card6 | 5 |  | Credit, debit, etc. |\n",
    "| addr1 | 332 | Yes | Billing region |\n",
    "| addr2 | 74 | Yes | Billing Country |\n",
    "| P_emaildomain | 59 |  | gmail.com, yahoo.com, etc. |\n",
    "| R_emaildomain | 60 |  | gmail.com, yahoo.com, etc. |\n",
    "| M{1-3, 5-9} | 3 | | Binary response (match or not) |\n",
    "| M4 | 4 | | Trinary Response (M0, M1, or M2) |\n",
    "\n",
    "Identity Table (not including NaN)\n",
    "| Attribute | Unique Values | Numerical | Notes |\n",
    "| --- | --- | --- | --- |\n",
    "| DeviceType | 2 | | desktop, mobile |\n",
    "| DeviceInfo | 1786 | | Freeform text entry (Windows, iOS Device, etc) |\n",
    "| id_12 | 2 | | NotFound, Found |\n",
    "| id_13 | 54 | Yes | |\n",
    "| id_14 | 25 | Yes | |\n",
    "| id_15 | 3 | | Found, New, Unknown (=/= NaN) |\n",
    "| id_16 | 2 |  | Found, NotFound |\n",
    "| id_17 | 104 | | Yes | |\n",
    "| id_{19-20} | {522, 394} | Yes | |\n",
    "| id_{28-29} | 2 | | Found, New/NotFound |\n",
    "| id_30 | 75 | | OS (with ver numbers) |\n",
    "| id_31 | 130 | | Browser OS |\n",
    "| id_32 | 4 | Yes | Seems to be either 24 or 32 so basically binary |\n",
    "| id_33 | 260 |  | Aspect ratio |\n",
    "| id_34 | 4 |  |  |\n",
    "| id_{35-38} | 2 | | Binary response |\n",
    "\n",
    "None of the above columns were removed during the 90% NaN drop.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function mapping booleans to integers (nullable)\n",
    "def mapper(input):\n",
    "    trues = ['T', 'Found', 'desktop']\n",
    "    falses = ['F', 'NotFound', 'New', 'mobile']\n",
    "\n",
    "    if input in trues:\n",
    "        return 1\n",
    "    elif input in falses:\n",
    "        return 0\n",
    "    return input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make booleans binary\n",
    "boolean_vars = ['DeviceType', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9', 'id_12', 'id_16', 'id_28', 'id_29', 'id_35', 'id_36', 'id_37', 'id_38']\n",
    "train[boolean_vars] = train[boolean_vars].applymap(mapper)\n",
    "test[boolean_vars] = test[boolean_vars].applymap(mapper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Feature engineering with `DeviceInfo` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`DeviceInfo` required the most specialised care as most values were simply `Windows` or `iOS` while Samsung and Android phones were split into more complex names, eg: `SM-G930V Build/NRD90M`. Before any encoding it was useful to try a version with engineering and one with direct frequency encoding to see which gave the best result. At the very least, grouping Samsung was likely to be interesting since it is a reputable brand."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Samsung replace (Narrows ~500 Results)\n",
    "train.loc[train['DeviceInfo'].fillna('None').str.contains('SM-'), 'DeviceInfo'] = 'SAMSUNG'\n",
    "test.loc[test['DeviceInfo'].fillna('None').str.contains('SM-'), 'DeviceInfo'] = 'SAMSUNG'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Feature engineering with `TransactionDT`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `TransctionDT` column allowed us to deduce the hour of the day, day of the week, and the time of day that a certain transaction occurred. This information was stored separately in new columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that will allow us to add the relevant features\n",
    "\n",
    "def new_vars1(train):\n",
    "    train['Hourofday'] = np.floor((train['TransactionDT'] / (60*60))) % 24\n",
    "    train['dayofweek'] = np.floor((train['TransactionDT'] / (60*60*24))) % 7\n",
    "    train['TimeInDay'] = train['TransactionDT'] % 86400\n",
    "    return train\n",
    "\n",
    "# Run the function on the train and test dataset\n",
    "train = new_vars1(train)\n",
    "test = new_vars1(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Feature engineering with `id`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The id_33 column holds information on the screen width and height combined with an x. We split this into two seperate columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function that wil allow us to split features within a single column\n",
    "def new_vars2(train):\n",
    "    train['screen_width'] = pd.to_numeric(train['id_33'].str.split('x', expand=True)[0])\n",
    "    train['screen_height'] = pd.to_numeric(train['id_33'].str.split('x', expand=True)[1])\n",
    "    return train\n",
    "\n",
    "# Run the function on the train and test dataset\n",
    "train = new_vars2(train)\n",
    "test = new_vars2(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Feature engineering with `card` and `addr` "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the Kaggle discussion section, \n",
    "1. `card1` may be understood as an encoded column of unique card identities due to the high variety of unique values and the column being defined as a categorical feature by the host of the competition.\n",
    "2. `card 2` will be assumed as the identity of bank/account as it represents a certain unique value of a transaction.\n",
    "3. `addr1` may be assumed as the country of origin due to the small number of encoded categorical values in the feature, and the level of concentration for each unique value.\n",
    "4. `addr2` may also be assumed as a county/state/province of the country as it holds a higher variety of unique categorical values with a higher value count for each unique value\n",
    "\n",
    "Using this discussion we carried out the following to extract extra information:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. We calculated how often certain values from the aforementioned columns appeared in the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to add features of unique value counts \n",
    "def new_vars3(train):\n",
    "    temp = train['card1'].value_counts().to_dict()\n",
    "    train['card1_counts'] = train['card1'].map(temp)\n",
    "    temp = train['card1'].value_counts().to_dict()\n",
    "    train['card2_counts'] = train['card1'].map(temp)\n",
    "    temp = train['addr1'].value_counts().to_dict()\n",
    "    train['addr1_counts'] = train['addr1'].map(temp)\n",
    "    temp = train['addr2'].value_counts().to_dict()\n",
    "    train['addr2_counts'] = train['addr2'].map(temp)\n",
    "    return train\n",
    "\n",
    "# Run the function on the train and test dataset\n",
    "train = new_vars3(train)\n",
    "test = new_vars3(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. We calculated the mean value of the transaction done by a certain uniquely identifiable card/address"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to add features of the mean amount being transferred for certain unique card/location identities\n",
    "def new_vars4(train):\n",
    "    temp = train.groupby(['card1'])['TransactionAmt'].mean().to_dict()\n",
    "    train['card1_mean'] = train['card1'].map(temp)\n",
    "    temp = train.groupby(['card2'])['TransactionAmt'].mean().to_dict()\n",
    "    train['card2_mean'] = train['card2'].map(temp)\n",
    "    temp = train.groupby(['addr1'])['TransactionAmt'].mean().to_dict()\n",
    "    train['addr1_mean'] = train['addr1'].map(temp)\n",
    "    temp = train.groupby(['addr2'])['TransactionAmt'].mean().to_dict()\n",
    "    train['addr2_mean'] = train['addr2'].map(temp)\n",
    "    return train\n",
    "\n",
    "# Run the function on the train and test dataset\n",
    "train = new_vars4(train)\n",
    "test = new_vars4(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. We calculated the ratio of the mean of the transaction compared to the corresponding transaction amount "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to add features of the ratio of certain transactions being made compared to the mean for each unique card/location identities\n",
    "def new_vars5(train):\n",
    "    train['card1_meanAMTratio'] = train['card1_mean']/train['TransactionAmt']\n",
    "    train['card2_meanAMTratio'] = train['card2_mean']/train['TransactionAmt']\n",
    "    train['addr1_meanAMTratio'] = train['addr1_mean']/train['TransactionAmt']\n",
    "    train['addr2_meanAMTratio'] = train['addr2_mean']/train['TransactionAmt']\n",
    "    return train\n",
    "\n",
    "# Run the function on the train and test dataset\n",
    "train = new_vars5(train)\n",
    "test = new_vars5(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "4. We calculated the standard deviation of the transactions for each unique card/location in order to assess how reguarly purchase habits are changing for each unique card/location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to add features of the standard deviation of certain transactions \n",
    "def new_vars6(train):\n",
    "    temp = train.groupby(['card1'])['TransactionAmt'].std().to_dict()\n",
    "    train['card1_sd'] = train['card1'].map(temp)\n",
    "    temp = train.groupby(['card2'])['TransactionAmt'].std().to_dict()\n",
    "    train['card2_sd'] = train['card2'].map(temp)\n",
    "    temp = train.groupby(['addr1'])['TransactionAmt'].std().to_dict()\n",
    "    train['addr1_sd'] = train['addr1'].map(temp)\n",
    "    temp = train.groupby(['addr2'])['TransactionAmt'].std().to_dict()\n",
    "    train['addr2_sd'] = train['addr2'].map(temp)\n",
    "    return train\n",
    "\n",
    "# Run the function on the train and test dataset\n",
    "train = new_vars6(train)\n",
    "test = new_vars6(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Other miscellaneous feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also added the following columns: one showing how many NAs are included in each row; one showing which transactions are recipients; and one extracting the cent value of all the transactions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to conduct the feature engineering\n",
    "def new_vars7(train):\n",
    "    train['isnotrecipient'] = train['R_emaildomain'].isna()\n",
    "    train['numberna'] = train.isnull().sum(axis=1)\n",
    "    train['cents'] = train['TransactionAmt'] % 1\n",
    "    return train\n",
    "\n",
    "# Run the function on the train and test dataset\n",
    "train = new_vars7(train)\n",
    "test = new_vars7(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.0 Categorical Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Options\n",
    "\n",
    "Frequency Encoding (FE), One Hot Encoding (OHE), and Ordinal Encoding (OE).\n",
    "| Attribute | Unique Values | Strategy |\n",
    "| --- | --- | --- |\n",
    "| ProductCD | 5 | **OHE** |\n",
    "| Card1 | 13552 | DROP or FE |\n",
    "| Card2 | 500 | FE |\n",
    "| Card3 | 114 | FE |\n",
    "| Card4 | 5 | OHE |\n",
    "| Card5 | 119 | FE |\n",
    "| Card6 | 5 | OHE |\n",
    "| addr1 | 332 | FE |\n",
    "| addr2 | 74 | **OHE** |\n",
    "| P_emaildomain | 59 | **FE** |\n",
    "| R_emaildomain | 60 | **FE** |\n",
    "| M{1-9} | 3-4 | **OHE** |\n",
    "| --- | --- | --- |\n",
    "| DeviceType | 2 | OHE |\n",
    "| DeviceInfo | 1786 | FE (Engineered) |\n",
    "| id_{12, 15-16, 28-29} | X | OHE |\n",
    "| id_{13-14, 17, 19-20} | X | FE |\n",
    "| id_30 | 75 | FE |\n",
    "| id_31 | 130 | FE (Engineered) |\n",
    "| id_32 | 4 | OHE |\n",
    "| id_33 | 130 | FE |\n",
    "| id_{34-38} | 2-4 | OHE |\n",
    "\n",
    "We decided email domains may be better to one hot encode.\n",
    "\n",
    "Boolean columns needed no encoding as they are already effectively numerical "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then applied the specific encoding techniques to each of the fields:\n",
    "\n",
    "1. One Hot Encoding\n",
    "2. Frequency Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FREQUENCY ENCODING\n",
    "def freq_encode(df, features):\n",
    "    \"\"\"\n",
    "    Frequency encodes selected columns in-places\n",
    "\n",
    "    features: List of frequency encoded feature names\n",
    "    \"\"\"\n",
    "    for f in features:\n",
    "        freqs = df[f].value_counts() / len(df) # Get frequencie\n",
    "        df[f] = df[f].apply(lambda x: freqs[x] if pd.notna(x) else x) # Apply to values\n",
    "\n",
    "# List of features to FE\n",
    "FE_ids = [f'id_{x}' for x in [13,14,17,19,20,30,31,33]]\n",
    "FE_vars = [\n",
    "    'card1',\n",
    "    'card2',\n",
    "    'card3',\n",
    "    'card5',\n",
    "    'addr1',\n",
    "    'P_emaildomain',\n",
    "    'R_emaildomain',\n",
    "    'DeviceInfo',\n",
    "]\n",
    "FE_vars.extend(FE_ids)\n",
    "\n",
    "# Frequency Encode\n",
    "freq_encode(train, FE_vars)\n",
    "freq_encode(test, FE_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ONE HOT ENCODING (using no NaN imputation)\n",
    "OHE_vars = [\n",
    "    'ProductCD',\n",
    "    'addr2',\n",
    "    'card4',\n",
    "    'card6',\n",
    "    'M4',\n",
    "    'id_15',\n",
    "    'id_32',\n",
    "    'id_34'\n",
    "]\n",
    "\n",
    "train = pd.get_dummies(train, columns=OHE_vars)\n",
    "test = pd.get_dummies(test, columns = OHE_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.0 Fix to the testing dataset (Dummy variables, and other columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Due to the inconsistency between the unique categorical variables that exist between the training and testing datasets as a result of columns removed during the intial data wrangling of the training dataset with dummy columns being produced, we excluded dummy variables that only exist in the testing dataset and created a column of 0s in the testing dataset for dummy columns that only exist in the training set. Addtionally, we removed any extra columns in the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correct the testing dataset \n",
    "only_train = set(train.columns) - set(test.columns) \n",
    "only_test = set(test.columns) - set(train.columns)\n",
    "only_test.remove('TransactionID')\n",
    "only_train.remove('isFraud')\n",
    "test = test.drop(only_test,axis = 1)\n",
    "for column in only_train:\n",
    "    test[column] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6.0 Model fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We downloaded the following necessary libraries to fit our models: XGBoostClassifer, HistGradientBoostingClassifier, and a LBMClassifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjf24\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\compat.py:36: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  from pandas import MultiIndex, Int64Index\n",
      "C:\\Users\\mjf24\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\experimental\\enable_hist_gradient_boosting.py:16: UserWarning: Since version 1.0, it is not needed to import enable_hist_gradient_boosting anymore. HistGradientBoostingClassifier and HistGradientBoostingRegressor are now stable and can be normally imported from sklearn.ensemble.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV, train_test_split\n",
    "from xgboost import XGBClassifier\n",
    "from sklearn.experimental import enable_hist_gradient_boosting\n",
    "from sklearn.ensemble import HistGradientBoostingClassifier\n",
    "from sklearn.metrics import roc_auc_score, accuracy_score, precision_score, recall_score\n",
    "import multiprocessing\n",
    "import lightgbm as gbm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.1 Fitting a XGBoostClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We began by fitting an XGBoostClassifier. This step was initialized by defining the independent and response variables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define independent and response variables\n",
    "X = train.drop('isFraud', axis=1)\n",
    "y = train['isFraud']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We took a smaller subsample of 15% in order to tune the hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a smaple to assess the accuracy score of the model \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.3, random_state = 30801)\n",
    "X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(X_train, y_train, train_size = 0.5, random_state = 30801)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then carried out a grid search on max_depth, n_estimators, learning_rate, colsample_bytree, and subsample, to tune the hyperparameters, and used a 2-fold crossvalidation to find the ideal values that minimizes the generalisation error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjf24\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "C:\\Users\\mjf24\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "C:\\Users\\mjf24\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.3,\n",
       " 'learning_rate': 0.3,\n",
       " 'max_depth': 9,\n",
       " 'n_estimators': 700,\n",
       " 'subsample': 1}"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# XGB Regressor(Multi-threaded)\n",
    "model = XGBClassifier(\n",
    "    n_jobs = multiprocessing.cpu_count() // 2, # Use half of the available CPU\n",
    "    use_label_encoder = False,\n",
    "    eval_metric = 'logloss'\n",
    ")\n",
    "\n",
    "# Params\n",
    "params = {\n",
    "    'max_depth': [9], # 3 - 10\n",
    "    'n_estimators': [700], # 100 - 1000\n",
    "    'learning_rate': [0.3], # 0.01 - 0.3\n",
    "    'colsample_bytree': [0.3], # 0.5 - 1\n",
    "    'subsample': [1] # 0.6 - 1\n",
    "}\n",
    "\n",
    "clf1 = GridSearchCV(model, params, verbose=0, cv=2)\n",
    "clf1.fit(X_train_sub, y_train_sub)\n",
    "clf1.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.1 Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the model we calculated the AUC score, as this is the same metric used by Kaggle, as well as the accuracy score, the precision score, and the recall score in order to obtain relatively simple metrics for clients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "              colsample_bynode=1, colsample_bytree=0.3,\n",
       "              enable_categorical=False, eval_metric='logloss', gamma=0,\n",
       "              gpu_id=-1, importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.04, max_delta_step=0, max_depth=9,\n",
       "              min_child_weight=1, missing=nan, monotone_constraints='()',\n",
       "              n_estimators=700, n_jobs=2, num_parallel_tree=1, predictor='auto',\n",
       "              random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n",
       "              subsample=1, tree_method='exact', use_label_encoder=False,\n",
       "              validate_parameters=1, verbosity=None)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define optimal model\n",
    "model = XGBClassifier(\n",
    "    n_jobs = multiprocessing.cpu_count() // 2, # Use half of the available CPU\n",
    "    use_label_encoder = False,\n",
    "    eval_metric = 'logloss',\n",
    "    max_depth = 9,\n",
    "    n_estimators = 700,\n",
    "    learning_rate = 0.04,\n",
    "    colsample_bytree = 0.3\n",
    ")\n",
    "\n",
    "# Fit model to the training data\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the optimal model and create predictions for the validation data\n",
    "y_prob_1 = model.predict_proba(X_test)\n",
    "y_pred_1 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9511223173129184"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the auc score\n",
    "roc_auc_score(y_test, y_prob_1[:, 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9810560794236752"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the accuracy score\n",
    "accuracy_score(y_test, y_pred_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.939314674339524"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the precision score\n",
    "precision_score(y_test, y_pred_1, average = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.493642174719912"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the recall score\n",
    "recall_score(y_test, y_pred_1, average = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.2 Variable importance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the 10 most imoportant variables in the xgboost model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Text(0.5, 1.0, '10 most important variables')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "variable_importance = [(X_train.columns[list(model.feature_importances_).index(i)],i) for i in \n",
    "sorted(model.feature_importances_, reverse = True)]\n",
    "variable_importance\n",
    "x_axis = []\n",
    "y_axis = []\n",
    "for i in range(10):\n",
    "    x_axis.append(variable_importance[i][0])\n",
    "    y_axis.append(variable_importance[i][1])\n",
    "sns.barplot(x=x_axis,y=y_axis).set(title='10 most important variables')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1.3 Fit the model to the full train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjf24\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n",
      "C:\\Users\\mjf24\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\xgboost\\data.py:262: FutureWarning: pandas.Int64Index is deprecated and will be removed from pandas in a future version. Use pandas.Index with the appropriate dtype instead.\n",
      "  elif isinstance(data.columns, (pd.Int64Index, pd.RangeIndex)):\n"
     ]
    }
   ],
   "source": [
    "# Fit the model with the entire training set and \n",
    "model.fit(train.drop(['isFraud','TransactionDT'],axis=1),train['isFraud'])\n",
    "y_pred_final_1 = model.predict_proba(test.drop(['TransactionID','TransactionDT'],axis=1))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submissionable dataset\n",
    "d_1 = {'TransactionID': test['TransactionID'], 'isFraud': y_pred_final_1}\n",
    "df_1 = pd.DataFrame(data = d_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Fitting a HistGradientBoostingClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much like the XGBoostClassifier, we began by tuning the hyperparameters. This step was initiated by defining samples and subsamples. However, for this model the `TransactionDT` column was dropped as this has proved to create a better fit for this model. This was not surprising as the rows of the column are almost all unique from each other. Moreover, although HistGradientBoostingClassifer is capable of handling NA entries in the dataset, we imputed the null values with -1 as this improved the score of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card5</th>\n",
       "      <th>addr1</th>\n",
       "      <th>dist1</th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>R_emaildomain</th>\n",
       "      <th>...</th>\n",
       "      <th>id_15_New</th>\n",
       "      <th>id_15_Unknown</th>\n",
       "      <th>id_32_0.0</th>\n",
       "      <th>id_32_16.0</th>\n",
       "      <th>id_32_24.0</th>\n",
       "      <th>id_32_32.0</th>\n",
       "      <th>id_34_match_status:-1</th>\n",
       "      <th>id_34_match_status:0</th>\n",
       "      <th>id_34_match_status:1</th>\n",
       "      <th>id_34_match_status:2</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>86400</td>\n",
       "      <td>68.50</td>\n",
       "      <td>0.000073</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.882729</td>\n",
       "      <td>0.000469</td>\n",
       "      <td>0.039079</td>\n",
       "      <td>19.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>86401</td>\n",
       "      <td>29.00</td>\n",
       "      <td>0.001157</td>\n",
       "      <td>0.005175</td>\n",
       "      <td>0.882729</td>\n",
       "      <td>0.049285</td>\n",
       "      <td>0.072393</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.386688</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>86469</td>\n",
       "      <td>59.00</td>\n",
       "      <td>0.001876</td>\n",
       "      <td>0.064593</td>\n",
       "      <td>0.882729</td>\n",
       "      <td>0.096759</td>\n",
       "      <td>0.044513</td>\n",
       "      <td>287.0</td>\n",
       "      <td>0.008629</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>86499</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.007127</td>\n",
       "      <td>0.010392</td>\n",
       "      <td>0.882729</td>\n",
       "      <td>0.043928</td>\n",
       "      <td>0.016050</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.170918</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>86506</td>\n",
       "      <td>50.00</td>\n",
       "      <td>0.000030</td>\n",
       "      <td>0.024623</td>\n",
       "      <td>0.882729</td>\n",
       "      <td>0.049285</td>\n",
       "      <td>0.006064</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.386688</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590535</th>\n",
       "      <td>15811047</td>\n",
       "      <td>49.00</td>\n",
       "      <td>0.002003</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>0.882729</td>\n",
       "      <td>0.502161</td>\n",
       "      <td>0.034106</td>\n",
       "      <td>48.0</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590536</th>\n",
       "      <td>15811049</td>\n",
       "      <td>39.50</td>\n",
       "      <td>0.000020</td>\n",
       "      <td>0.012607</td>\n",
       "      <td>0.882729</td>\n",
       "      <td>0.138031</td>\n",
       "      <td>0.071155</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.386688</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590537</th>\n",
       "      <td>15811079</td>\n",
       "      <td>30.95</td>\n",
       "      <td>0.001168</td>\n",
       "      <td>0.001243</td>\n",
       "      <td>0.882729</td>\n",
       "      <td>0.138031</td>\n",
       "      <td>0.012878</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.386688</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590538</th>\n",
       "      <td>15811088</td>\n",
       "      <td>117.00</td>\n",
       "      <td>0.005090</td>\n",
       "      <td>0.010729</td>\n",
       "      <td>0.882729</td>\n",
       "      <td>0.138031</td>\n",
       "      <td>0.013864</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.047904</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>590539</th>\n",
       "      <td>15811131</td>\n",
       "      <td>279.95</td>\n",
       "      <td>0.013454</td>\n",
       "      <td>0.030843</td>\n",
       "      <td>0.882729</td>\n",
       "      <td>0.049285</td>\n",
       "      <td>0.078462</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.386688</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>590540 rows  537 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TransactionDT  TransactionAmt     card1     card2     card3     card5  \\\n",
       "0               86400           68.50  0.000073 -1.000000  0.882729  0.000469   \n",
       "1               86401           29.00  0.001157  0.005175  0.882729  0.049285   \n",
       "2               86469           59.00  0.001876  0.064593  0.882729  0.096759   \n",
       "3               86499           50.00  0.007127  0.010392  0.882729  0.043928   \n",
       "4               86506           50.00  0.000030  0.024623  0.882729  0.049285   \n",
       "...               ...             ...       ...       ...       ...       ...   \n",
       "590535       15811047           49.00  0.002003 -1.000000  0.882729  0.502161   \n",
       "590536       15811049           39.50  0.000020  0.012607  0.882729  0.138031   \n",
       "590537       15811079           30.95  0.001168  0.001243  0.882729  0.138031   \n",
       "590538       15811088          117.00  0.005090  0.010729  0.882729  0.138031   \n",
       "590539       15811131          279.95  0.013454  0.030843  0.882729  0.049285   \n",
       "\n",
       "           addr1  dist1  P_emaildomain  R_emaildomain  ...  id_15_New  \\\n",
       "0       0.039079   19.0      -1.000000           -1.0  ...          0   \n",
       "1       0.072393   -1.0       0.386688           -1.0  ...          0   \n",
       "2       0.044513  287.0       0.008629           -1.0  ...          0   \n",
       "3       0.016050   -1.0       0.170918           -1.0  ...          0   \n",
       "4       0.006064   -1.0       0.386688           -1.0  ...          1   \n",
       "...          ...    ...            ...            ...  ...        ...   \n",
       "590535  0.034106   48.0      -1.000000           -1.0  ...          0   \n",
       "590536  0.071155   -1.0       0.386688           -1.0  ...          0   \n",
       "590537  0.012878   -1.0       0.386688           -1.0  ...          0   \n",
       "590538  0.013864    3.0       0.047904           -1.0  ...          0   \n",
       "590539  0.078462   -1.0       0.386688           -1.0  ...          0   \n",
       "\n",
       "        id_15_Unknown  id_32_0.0  id_32_16.0  id_32_24.0  id_32_32.0  \\\n",
       "0                   0          0           0           0           0   \n",
       "1                   0          0           0           0           0   \n",
       "2                   0          0           0           0           0   \n",
       "3                   0          0           0           0           0   \n",
       "4                   0          0           0           0           1   \n",
       "...               ...        ...         ...         ...         ...   \n",
       "590535              0          0           0           0           0   \n",
       "590536              0          0           0           0           0   \n",
       "590537              0          0           0           0           0   \n",
       "590538              0          0           0           0           0   \n",
       "590539              0          0           0           0           0   \n",
       "\n",
       "        id_34_match_status:-1  id_34_match_status:0  id_34_match_status:1  \\\n",
       "0                           0                     0                     0   \n",
       "1                           0                     0                     0   \n",
       "2                           0                     0                     0   \n",
       "3                           0                     0                     0   \n",
       "4                           0                     0                     0   \n",
       "...                       ...                   ...                   ...   \n",
       "590535                      0                     0                     0   \n",
       "590536                      0                     0                     0   \n",
       "590537                      0                     0                     0   \n",
       "590538                      0                     0                     0   \n",
       "590539                      0                     0                     0   \n",
       "\n",
       "        id_34_match_status:2  \n",
       "0                          0  \n",
       "1                          0  \n",
       "2                          0  \n",
       "3                          0  \n",
       "4                          1  \n",
       "...                      ...  \n",
       "590535                     0  \n",
       "590536                     0  \n",
       "590537                     0  \n",
       "590538                     0  \n",
       "590539                     0  \n",
       "\n",
       "[590540 rows x 537 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impute the training dataset \n",
    "with -1\n",
    "X.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TransactionID</th>\n",
       "      <th>TransactionDT</th>\n",
       "      <th>TransactionAmt</th>\n",
       "      <th>card1</th>\n",
       "      <th>card2</th>\n",
       "      <th>card3</th>\n",
       "      <th>card5</th>\n",
       "      <th>addr1</th>\n",
       "      <th>dist1</th>\n",
       "      <th>P_emaildomain</th>\n",
       "      <th>...</th>\n",
       "      <th>addr2_83.0</th>\n",
       "      <th>addr2_63.0</th>\n",
       "      <th>addr2_49.0</th>\n",
       "      <th>card6_debit or credit</th>\n",
       "      <th>addr2_92.0</th>\n",
       "      <th>addr2_35.0</th>\n",
       "      <th>addr2_21.0</th>\n",
       "      <th>addr2_14.0</th>\n",
       "      <th>addr2_23.0</th>\n",
       "      <th>id_32_0.0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3663549</td>\n",
       "      <td>18403224</td>\n",
       "      <td>31.950</td>\n",
       "      <td>0.000148</td>\n",
       "      <td>0.073706</td>\n",
       "      <td>0.859613</td>\n",
       "      <td>0.507195</td>\n",
       "      <td>0.003570</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.409417</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3663550</td>\n",
       "      <td>18403263</td>\n",
       "      <td>49.000</td>\n",
       "      <td>0.001897</td>\n",
       "      <td>0.073706</td>\n",
       "      <td>0.859613</td>\n",
       "      <td>0.507195</td>\n",
       "      <td>0.076398</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.047461</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3663551</td>\n",
       "      <td>18403310</td>\n",
       "      <td>171.000</td>\n",
       "      <td>0.000065</td>\n",
       "      <td>0.000150</td>\n",
       "      <td>0.859613</td>\n",
       "      <td>0.507195</td>\n",
       "      <td>0.014261</td>\n",
       "      <td>2635.0</td>\n",
       "      <td>0.079731</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3663552</td>\n",
       "      <td>18403310</td>\n",
       "      <td>284.950</td>\n",
       "      <td>0.001656</td>\n",
       "      <td>0.022384</td>\n",
       "      <td>0.859613</td>\n",
       "      <td>0.090371</td>\n",
       "      <td>0.010460</td>\n",
       "      <td>17.0</td>\n",
       "      <td>0.409417</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3663553</td>\n",
       "      <td>18403317</td>\n",
       "      <td>67.950</td>\n",
       "      <td>0.002402</td>\n",
       "      <td>0.002629</td>\n",
       "      <td>0.859613</td>\n",
       "      <td>0.041682</td>\n",
       "      <td>0.064556</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0.409417</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506686</th>\n",
       "      <td>4170235</td>\n",
       "      <td>34214279</td>\n",
       "      <td>94.679</td>\n",
       "      <td>0.004902</td>\n",
       "      <td>0.013191</td>\n",
       "      <td>0.105812</td>\n",
       "      <td>0.141301</td>\n",
       "      <td>0.002246</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.409417</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506687</th>\n",
       "      <td>4170236</td>\n",
       "      <td>34214287</td>\n",
       "      <td>12.173</td>\n",
       "      <td>0.009155</td>\n",
       "      <td>0.018054</td>\n",
       "      <td>0.105812</td>\n",
       "      <td>0.141301</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.079731</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506688</th>\n",
       "      <td>4170237</td>\n",
       "      <td>34214326</td>\n",
       "      <td>49.000</td>\n",
       "      <td>0.000989</td>\n",
       "      <td>0.063848</td>\n",
       "      <td>0.859613</td>\n",
       "      <td>0.507195</td>\n",
       "      <td>0.014760</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.079731</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506689</th>\n",
       "      <td>4170238</td>\n",
       "      <td>34214337</td>\n",
       "      <td>202.000</td>\n",
       "      <td>0.000008</td>\n",
       "      <td>0.000249</td>\n",
       "      <td>0.859613</td>\n",
       "      <td>0.141301</td>\n",
       "      <td>0.007804</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.079731</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>506690</th>\n",
       "      <td>4170239</td>\n",
       "      <td>34214345</td>\n",
       "      <td>24.346</td>\n",
       "      <td>0.000120</td>\n",
       "      <td>0.000341</td>\n",
       "      <td>0.002726</td>\n",
       "      <td>0.000811</td>\n",
       "      <td>-1.000000</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>0.079731</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>506691 rows  538 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        TransactionID  TransactionDT  TransactionAmt     card1     card2  \\\n",
       "0             3663549       18403224          31.950  0.000148  0.073706   \n",
       "1             3663550       18403263          49.000  0.001897  0.073706   \n",
       "2             3663551       18403310         171.000  0.000065  0.000150   \n",
       "3             3663552       18403310         284.950  0.001656  0.022384   \n",
       "4             3663553       18403317          67.950  0.002402  0.002629   \n",
       "...               ...            ...             ...       ...       ...   \n",
       "506686        4170235       34214279          94.679  0.004902  0.013191   \n",
       "506687        4170236       34214287          12.173  0.009155  0.018054   \n",
       "506688        4170237       34214326          49.000  0.000989  0.063848   \n",
       "506689        4170238       34214337         202.000  0.000008  0.000249   \n",
       "506690        4170239       34214345          24.346  0.000120  0.000341   \n",
       "\n",
       "           card3     card5     addr1   dist1  P_emaildomain  ...  addr2_83.0  \\\n",
       "0       0.859613  0.507195  0.003570     1.0       0.409417  ...           0   \n",
       "1       0.859613  0.507195  0.076398     4.0       0.047461  ...           0   \n",
       "2       0.859613  0.507195  0.014261  2635.0       0.079731  ...           0   \n",
       "3       0.859613  0.090371  0.010460    17.0       0.409417  ...           0   \n",
       "4       0.859613  0.041682  0.064556     6.0       0.409417  ...           0   \n",
       "...          ...       ...       ...     ...            ...  ...         ...   \n",
       "506686  0.105812  0.141301  0.002246    -1.0       0.409417  ...           0   \n",
       "506687  0.105812  0.141301 -1.000000    -1.0       0.079731  ...           0   \n",
       "506688  0.859613  0.507195  0.014760    -1.0       0.079731  ...           0   \n",
       "506689  0.859613  0.141301  0.007804    -1.0       0.079731  ...           0   \n",
       "506690  0.002726  0.000811 -1.000000    -1.0       0.079731  ...           0   \n",
       "\n",
       "        addr2_63.0  addr2_49.0  card6_debit or credit  addr2_92.0  addr2_35.0  \\\n",
       "0                0           0                      0           0           0   \n",
       "1                0           0                      0           0           0   \n",
       "2                0           0                      0           0           0   \n",
       "3                0           0                      0           0           0   \n",
       "4                0           0                      0           0           0   \n",
       "...            ...         ...                    ...         ...         ...   \n",
       "506686           0           0                      0           0           0   \n",
       "506687           0           0                      0           0           0   \n",
       "506688           0           0                      0           0           0   \n",
       "506689           0           0                      0           0           0   \n",
       "506690           0           0                      0           0           0   \n",
       "\n",
       "        addr2_21.0  addr2_14.0  addr2_23.0  id_32_0.0  \n",
       "0                0           0           0          0  \n",
       "1                0           0           0          0  \n",
       "2                0           0           0          0  \n",
       "3                0           0           0          0  \n",
       "4                0           0           0          0  \n",
       "...            ...         ...         ...        ...  \n",
       "506686           0           0           0          0  \n",
       "506687           0           0           0          0  \n",
       "506688           0           0           0          0  \n",
       "506689           0           0           0          0  \n",
       "506690           0           0           0          0  \n",
       "\n",
       "[506691 rows x 538 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impute the training datset with -1\n",
    "test.fillna(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a subsample to tune hyperparameters\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size = 0.3, random_state = 30801)\n",
    "X_train_sub, X_test_sub, y_train_sub, y_test_sub = train_test_split(X_train.drop('TransactionDT', axis = 1), y_train, train_size = 0.4, random_state = 30801)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.06, 'max_iter': 70, 'max_leaf_nodes': 620}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Define a default model to conduct a grid searh\n",
    "model = HistGradientBoostingClassifier(learning_rate = 0.1, max_leaf_nodes = 31)\n",
    "\n",
    "# Define the grid of parameters we wish to search on\n",
    "param_grid = {\n",
    "    'learning_rate': [0.06], # [0.01, 0.02, 0.05, 0.06, 0.07, 0.1, 0.3, 0.4]\n",
    "    'max_leaf_nodes' : [620], # [500, 550, 600, 610, 620, 650, 700, 800]\n",
    "    'max_iter': [70] # [50, 60, 70, 80, 90, 100]\n",
    "}\n",
    "\n",
    "# Conduct a grid search and return the best values\n",
    "clf2 = GridSearchCV(estimator = model, param_grid = param_grid, cv = 2, n_jobs = 4, verbose = 0)\n",
    "clf2.fit(X_train_sub, y_train_sub)\n",
    "clf2.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.1 Evaluating Accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the results from above fitted these hyperparameters to the model and evaluated using the same metrics previously mentioned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HistGradientBoostingClassifier(learning_rate=0.06, max_iter=70,\n",
       "                               max_leaf_nodes=620)"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model with tuned hyperparas\n",
    "model = HistGradientBoostingClassifier(learning_rate = 0.06, max_leaf_nodes = 620, max_iter = 70)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the optimal model and create predictions for the validation data\n",
    "y_prob_2 = model.predict_proba(X_test)\n",
    "y_pred_2 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9412369333362397"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the auc score\n",
    "roc_auc_score(y_test, y_prob_2[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9801997203528006"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the accuracy score\n",
    "accuracy_score(y_test, y_pred_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9272287862513426"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the precision score\n",
    "precision_score(y_test, y_pred_2, average = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4746717987490549"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the recall score\n",
    "recall_score(y_test, y_pred_2, average = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2.2 Fit the model to the full train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mjf24\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\sklearn\\base.py:493: FutureWarning: The feature names should match those that were passed during fit. Starting version 1.2, an error will be raised.\n",
      "Feature names must be in the same order as they were in fit.\n",
      "\n",
      "  warnings.warn(message, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Define final model by fitting with tuned hyperparas and fit to the whole dataset\n",
    "model.fit(train.drop(['isFraud', 'TransactionDT'], axis = 1), train['isFraud'])\n",
    "y_pred_final_2 = model.predict_proba(test.drop(['TransactionID','TransactionDT'],axis=1))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submissionable dataset\n",
    "d_2 = {'TransactionID': test['TransactionID'], 'isFraud': y_pred_final_2}\n",
    "df_2 = pd.DataFrame(data = d_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.3 Fitting a LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section we fitted the dataset to a LGBMClassifier. Much like the HistGradientBoostingClassifier we dropped the `TransactionDT` column as it proved to be a better fit.  Again, we imputated -1 for NaN entries as this had a better performance score, despite the model being able to handle null values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we began, we coverted float64, and int64 to float32 and int32 to decrease memory usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save memory usage in the training dataset\n",
    "for col in train.columns:\n",
    "    if train[col].dtype=='float64': train[col] = train[col].astype('float32')\n",
    "    if train[col].dtype=='int64': train[col] = train[col].astype('int32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save memory usage in the testing dataset\n",
    "for col in test.columns:\n",
    "    if test[col].dtype=='float64': test[col] = test[col].astype('float32')\n",
    "    if test[col].dtype=='int64': test[col] = test[col].astype('int32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lightgbm cannot handle ':' in column names from the dummy variables so these were removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove : in the training columns\n",
    "for i in train.columns:\n",
    "    if ':' in i:\n",
    "        train.rename(columns = {i:i.replace(':', '') }, inplace = True)\n",
    "for i in X_train.columns:\n",
    "    if ':' in i:\n",
    "        X_train.rename(columns = {i:i.replace(':', '') }, inplace = True)\n",
    "\n",
    "# Remove : in the testing columns\n",
    "for i in test.columns:\n",
    "    if ':' in i:\n",
    "        test.rename(columns = {i:i.replace(':', '') }, inplace = True)\n",
    "for i in X_test.columns:\n",
    "    if ':' in i:\n",
    "        X_test.rename(columns = {i:i.replace(':', '') }, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using optuna we were able to tune the best hyperparameters for this model and the hyperparameters were as shown below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.1 Evaluating Accuracy of the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the results from above we fitted these hyperparameters to the model and evaluated using the same metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LGBMClassifier(colsample_bytree=0.5, learning_rate=0.016, min_child_samples=100,\n",
       "               n_estimators=1000, num_leaves=4000, random_state=4840)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit model with the tuned hyperparas\n",
    "model = gbm.LGBMClassifier(n_estimators = 1000,num_leaves = 4000 ,\n",
    "                                    learning_rate = 0.016,min_child_samples = 100,\n",
    "                           colsample_bytree = 0.5,\n",
    "                                   random_state = 4840)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create predicted values with the fitted model\n",
    "y_prob_3 = model.predict_proba(X_test)\n",
    "y_pred_3 = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9541142101433764"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the auc score\n",
    "roc_auc_score(y_test, y_prob_3[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9830010305337972"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the accuracy score\n",
    "accuracy_score(y_test, y_pred_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9565428502063608"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the precision score\n",
    "precision_score(y_test, y_pred_3, average = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5416179806172245"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the recall score\n",
    "recall_score(y_test, y_pred_3, average = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.2 Fit the model to the full train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model with the whole dataset and make prediction values\n",
    "model.fit(train.drop(['isFraud','TransactionDT'],axis=1),train['isFraud'])\n",
    "y_pred_final_3= model.predict_proba(test.drop(['TransactionID','TransactionDT'],axis=1))[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submissionable dataset\n",
    "d_3 = {'TransactionID': test['TransactionID'], 'isFraud': y_pred_final_3}\n",
    "df_3 = pd.DataFrame(data = d_3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.0 Final fitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our final model is an ensemble of all three models with weights based on the AUC score obtained from our Kaggle public score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.1 Evaluate Ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We used the same evaluations to assess the quality of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create an ensemble model using the results\n",
    "ensemble = (y_pred_1*0.923 + y_pred_2*0.929 + y_pred_3*0.93)/(0.923 + 0.929 + 0.93)\n",
    "ensemble_prob = (y_prob_1*0.923 + y_prob_2*0.929 + y_prob_3*0.93)/(0.923 + 0.929 + 0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Round the ensemble predictions to the binary domain of integers [0, 1]\n",
    "y_pred_4 = np.rint(ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.952197808241645"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the auc score\n",
    "roc_auc_score(y_test, ensemble_prob[:,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9816294045643454"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the accuracy score\n",
    "accuracy_score(y_test, y_pred_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9481891996391287"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the precision score\n",
    "precision_score(y_test, y_pred_4, average = 'binary')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5056704928173757"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the recall score\n",
    "recall_score(y_test, y_pred_4, average = 'binary')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7.2 Fit final ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, our final model was fitted using the aforementioned method involving the weights to create an ensemble model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a column of the isFraud results and calculate the weighted values\n",
    "ensemble_res = (df_1['isFraud']*0.923 + df_2['isFraud']*0.929 + df_3['isFraud']*0.93)/(0.923+0.929+0.93)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dataset of the final ensemble results\n",
    "d_4 = {'TransactionID': test['TransactionID'], 'isFraud': ensemble_res}\n",
    "df_4 = pd.DataFrame(data = d_4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a submissable file and export it to a seperate csv file\n",
    "df_4.to_csv('Final_sub.csv', index = False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1428bb0e3b7a6d9f7eb0ed1a64ca195ed990863c531cf88b6c2baf5efd94a875"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
